{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Huggingface Sagemaker-sdk - Run a batch transform inference job with 🤗 Transformers\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to this getting started guide, we will use the new Hugging Face Inference DLCs and Amazon SageMaker Python SDK to deploy two transformer model for inference. \n",
    "In this example we run a batch-transform job using a trained Hugging Face Transformer model on to SageMaker for inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Batch Transform after training a model \n",
    "_not included in the notebook_\n",
    "\n",
    "After you train a model, you can use [Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) to perform inferences with the model. In Batch Transform you provide your inference data as a S3 uri and SageMaker will care of downloading it, running the prediction and uploading the results afterwards to S3 again. You can find more documentation for Batch Transform [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n",
    "\n",
    "If you trained the model using the **HuggingFace estimator**, you can invoke `transformer()` method to create a transform job for a model based on the training job.\n",
    "\n",
    "```python\n",
    "batch_job = huggingface_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "\n",
    "batch_job.transform(\n",
    "    data='s3://s3-uri-to-batch-data',\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')\n",
    "```\n",
    "For more details about what can be specified here, see [API docs](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform).\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade\n",
    "# !pip install torch -q\n",
    "!pip install transformers -q"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Batch Transform Inference Job with a fine-tuned model using `jsonl`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "In this example we are using a excerpt of provided `test.csv` from the first session today. The `csv` contains ~1000 tweets. The `csv` contains 2 columns `\"text\"` and `\"label\"`. To use this `csv` we need to convert it into a `jsonl` file and upload it to s3. Due to the complex structure of text are only `jsonl` file supported for batch transform. As pre-processing we are removing the `@anonymized_account` of the tweet.\n",
    "\n",
    "_**NOTE**: While preprocessing you need to make sure that your `inputs` fit the `max_length`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import csv\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "# get the s3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# datset files\n",
    "dataset_csv_file=\"test.csv\"\n",
    "dataset_json_file=\"transform_dataset.json\"\n",
    "\n",
    "with open(dataset_csv_file, \"r+\") as infile, open(dataset_json_file, \"w+\", encoding='utf8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        # remove @\n",
    "        row[\"inputs\"] = row[\"text\"].replace(\"@anonymized_account\",\"\").strip()\n",
    "        del row[\"label\"]\n",
    "        del row[\"text\"]\n",
    "        json.dump(row, outfile,ensure_ascii=False)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "                \n",
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_json_file,input_s3_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The created file looks like this\n",
    "\n",
    "```json\n",
    "{\"inputs\": \"Spoko, jak im Duda z Morawieckim zamówią po pięć piw to wszystko będzie ok.\"}\n",
    "{\"inputs\": \"Ale on tu nie miał szans jej zagrania, a ta 'proba' to czysta prowizorka.\"}\n",
    "{\"inputs\": \"No czy Prezes nie miał racji, mówiąc,ze to są zdradzieckie mordy? No czy nie miał racji?😁😁\"}\n",
    "{\"inputs\": \"Przecież to nawet nie jest przewrotka 😂\"}\n",
    "{\"inputs\": \"Owszem podatki tak. Ale nie w takich okolicznościach. Czemu Małysza odpalili z teamu Orlen?\"}\n",
    "{\"inputs\": \"skąd wiesz jaki Skendija ma budżet skoro mówisz że jest bogatsza ? Tylko dwóch zawodników ponoć dobrze zarabia.\"}\n",
    "{\"inputs\": \"Z tego, co widzę, to kibice Widzewa mają szczęście, że trwa mundial. Dzięki temu ogólnopolska szydera jest tylko z Argentyny i Messiego.\"}\n",
    "{\"inputs\": \"Na utrzymanie własnej armii 2% PKB, tyle że teraz to jedna wielka ściema\"}\n",
    "{\"inputs\": \"Przypomnijcie mi ze muszę jeszcze suszarkę spakować\"}\n",
    "....\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Inference Transformer to run the batch job\n",
    "\n",
    "We use the [polish_cyberbullying_bert_base](https://colab.research.google.com/drive/1KlpbVkAo1cTX3u5mrI7Bj-VfaX4sUtuR?usp=sharing) trained in the earlier session today by Dogu. If you haven't attempted you can re-do the session with this notebook: https://colab.research.google.com/drive/1KlpbVkAo1cTX3u5mrI7Bj-VfaX4sUtuR?usp=sharingmodel\n",
    "Or use the model as it is provided on Amazon S3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**using the S3 provided model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# download the model from S3 and unzip\n",
    "!wget https://pai-marketplace21-tutorials.s3.amazonaws.com/practical-nlp/models/polish_cyberbullying_bert_base.tgz\n",
    "!tar zxvf polish_cyberbullying_bert_base.tgz\n",
    "\n",
    "# package pre-trained model into .tar.gz format\n",
    "!cd polish_cyberbullying_bert_base && tar zcvf model.tar.gz * \n",
    "!mv polish_cyberbullying_bert_base/model.tar.gz ./model.tar.gz"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-09-30 09:19:36--  https://pai-marketplace21-tutorials.s3.amazonaws.com/practical-nlp/models/polish_cyberbullying_bert_base.tgz\n",
      "Auflösen des Hostnamens pai-marketplace21-tutorials.s3.amazonaws.com (pai-marketplace21-tutorials.s3.amazonaws.com)… 52.216.131.131\n",
      "Verbindungsaufbau zu pai-marketplace21-tutorials.s3.amazonaws.com (pai-marketplace21-tutorials.s3.amazonaws.com)|52.216.131.131|:443 … verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet … 200 OK\n",
      "Länge: 434163450 (414M) [application/x-gzip]\n",
      "Wird in »polish_cyberbullying_bert_base.tgz.1« gespeichert.\n",
      "\n",
      "polish_cyberbullyin 100%[===================>] 414,05M  9,50MB/s    in 22s     \n",
      "\n",
      "2021-09-30 09:19:58 (18,8 MB/s) - »polish_cyberbullying_bert_base.tgz.1« gespeichert [434163450/434163450]\n",
      "\n",
      "x polish_cyberbullying_bert_base/\n",
      "x polish_cyberbullying_bert_base/added_tokens.json\n",
      "x polish_cyberbullying_bert_base/special_tokens_map.json\n",
      "x polish_cyberbullying_bert_base/config.json\n",
      "x polish_cyberbullying_bert_base/tokenizer.json\n",
      "x polish_cyberbullying_bert_base/merges.txt\n",
      "x polish_cyberbullying_bert_base/pytorch_model.bin\n",
      "a added_tokens.json\n",
      "a config.json\n",
      "a merges.txt\n",
      "a pytorch_model.bin\n",
      "a special_tokens_map.json\n",
      "a tokenizer.json\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**using the Hugging Face model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Download Hugging Face Pretrained Model\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# MODEL = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "# model.save_pretrained('my_model')\n",
    "# tokenizer.save_pretrained('my_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# package the inference scrip and pre-trained model into .tar.gz format\n",
    "# !cd my_model && tar zcvf model.tar.gz * \n",
    "# !mv my_model/model.tar.gz ./model.tar.gz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**upload the mmodel to Amazon S3**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# upload pre-trained model to s3 bucket\n",
    "model_url = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/model\")\n",
    "print(f\"Uploading Model to {model_url}\")\n",
    "model_uri = S3Uploader.upload('model.tar.gz',model_url)\n",
    "print(f\"Uploaded model to {model_uri}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Batch Transform job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_uri, # configuration for loading model from Hub\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "    name=\"prosus-workshop-bert-batch\", # model and batch name\n",
    "   py_version='py36', # python version used\n",
    ")\n",
    "\n",
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=input_s3_path,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use batch transform when you:\n",
    "\n",
    "* Want to get inferences for an entire dataset and index them to serve inferences in real time\n",
    "* Don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "* Don't need the subsecond latency that SageMaker hosted endpoints provide\n",
    "\n",
    "You can also use batch transform to preprocess your data before using it to train a new model or generate inferences.\n",
    "The following diagram shows the workflow of a batch transform job:\n",
    "\n",
    "![batch-transform](../../imgs/batch-transform-v2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from ast import literal_eval\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"tweet_data.json.out\"\n",
    "output_path = s3_path_join(output_s3_path,output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')\n",
    "\n",
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "        \n",
    "# print results \n",
    "print(batch_transform_result[:20])"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}