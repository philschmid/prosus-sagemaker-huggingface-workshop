{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Huggingface Sagemaker-sdk - Run a batch transform inference job with ðŸ¤— Transformers\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to this getting started guide, we will use the new Hugging Face Inference DLCs and Amazon SageMaker Python SDK to deploy two transformer model for inference. \n",
    "In this example we run a batch-transform job using a trained Hugging Face Transformer model on to SageMaker for inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Batch Transform after training a model \n",
    "_not included in the notebook_\n",
    "\n",
    "After you train a model, you can use [Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) to perform inferences with the model. In Batch Transform you provide your inference data as a S3 uri and SageMaker will care of downloading it, running the prediction and uploading the results afterwards to S3 again. You can find more documentation for Batch Transform [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n",
    "\n",
    "If you trained the model using the **HuggingFace estimator**, you can invoke `transformer()` method to create a transform job for a model based on the training job.\n",
    "\n",
    "```python\n",
    "batch_job = huggingface_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "\n",
    "batch_job.transform(\n",
    "    data='s3://s3-uri-to-batch-data',\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')\n",
    "```\n",
    "For more details about what can be specified here, see [API docs](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform).\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade\n",
    "# !pip install torch -q\n",
    "!pip install transformers -q"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Batch Transform Inference Job with a fine-tuned model using `jsonl`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "In this example we are using a excerpt of provided `test.csv` from the first session today. The `csv` contains ~1000 tweets. The `csv` contains 2 columns `\"text\"` and `\"label\"`. To use this `csv` we need to convert it into a `jsonl` file and upload it to s3. Due to the complex structure of text are only `jsonl` file supported for batch transform. As pre-processing we are removing the `@anonymized_account` of the tweet.\n",
    "\n",
    "_**NOTE**: While preprocessing you need to make sure that your `inputs` fit the `max_length`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import csv\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "# get the s3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# datset files\n",
    "dataset_csv_file=\"test.csv\"\n",
    "dataset_json_file=\"transform_dataset.json\"\n",
    "\n",
    "with open(dataset_csv_file, \"r+\") as infile, open(dataset_json_file, \"w+\", encoding='utf8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        # remove @\n",
    "        row[\"inputs\"] = row[\"text\"].replace(\"@anonymized_account\",\"\").strip()\n",
    "        del row[\"label\"]\n",
    "        del row[\"text\"]\n",
    "        json.dump(row, outfile,ensure_ascii=False)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "                \n",
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_json_file,input_s3_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The created file looks like this\n",
    "\n",
    "```json\n",
    "{\"inputs\": \"Spoko, jak im Duda z Morawieckim zamÃ³wiÄ… po piÄ™Ä‡ piw to wszystko bÄ™dzie ok.\"}\n",
    "{\"inputs\": \"Ale on tu nie miaÅ‚ szans jej zagrania, a ta 'proba' to czysta prowizorka.\"}\n",
    "{\"inputs\": \"No czy Prezes nie miaÅ‚ racji, mÃ³wiÄ…c,ze to sÄ… zdradzieckie mordy? No czy nie miaÅ‚ racji?ðŸ˜ðŸ˜\"}\n",
    "{\"inputs\": \"PrzecieÅ¼ to nawet nie jest przewrotka ðŸ˜‚\"}\n",
    "{\"inputs\": \"Owszem podatki tak. Ale nie w takich okolicznoÅ›ciach. Czemu MaÅ‚ysza odpalili z teamu Orlen?\"}\n",
    "{\"inputs\": \"skÄ…d wiesz jaki Skendija ma budÅ¼et skoro mÃ³wisz Å¼e jest bogatsza ? Tylko dwÃ³ch zawodnikÃ³w ponoÄ‡ dobrze zarabia.\"}\n",
    "{\"inputs\": \"Z tego, co widzÄ™, to kibice Widzewa majÄ… szczÄ™Å›cie, Å¼e trwa mundial. DziÄ™ki temu ogÃ³lnopolska szydera jest tylko z Argentyny i Messiego.\"}\n",
    "{\"inputs\": \"Na utrzymanie wÅ‚asnej armii 2% PKB, tyle Å¼e teraz to jedna wielka Å›ciema\"}\n",
    "{\"inputs\": \"Przypomnijcie mi ze muszÄ™ jeszcze suszarkÄ™ spakowaÄ‡\"}\n",
    "....\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Inference Transformer to run the batch job\n",
    "\n",
    "We use the [polish_cyberbullying_bert_base](https://colab.research.google.com/drive/1KlpbVkAo1cTX3u5mrI7Bj-VfaX4sUtuR?usp=sharing) trained in the earlier session today by Dogu. If you haven't attempted you can re-do the session with this notebook: https://colab.research.google.com/drive/1KlpbVkAo1cTX3u5mrI7Bj-VfaX4sUtuR?usp=sharingmodel\n",
    "Or use the model as it is provided on Amazon S3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**using the S3 provided model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# download the model from S3 and unzip\n",
    "!wget https://pai-marketplace21-tutorials.s3.amazonaws.com/practical-nlp/models/polish_cyberbullying_bert_base.tgz\n",
    "!tar zxvf polish_cyberbullying_bert_base.tgz\n",
    "\n",
    "# package pre-trained model into .tar.gz format\n",
    "!cd polish_cyberbullying_bert_base && tar zcvf model.tar.gz * \n",
    "!mv polish_cyberbullying_bert_base/model.tar.gz ./model.tar.gz"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-09-30 09:19:36--  https://pai-marketplace21-tutorials.s3.amazonaws.com/practical-nlp/models/polish_cyberbullying_bert_base.tgz\n",
      "AuflÃ¶sen des Hostnamens pai-marketplace21-tutorials.s3.amazonaws.com (pai-marketplace21-tutorials.s3.amazonaws.com)â€¦ 52.216.131.131\n",
      "Verbindungsaufbau zu pai-marketplace21-tutorials.s3.amazonaws.com (pai-marketplace21-tutorials.s3.amazonaws.com)|52.216.131.131|:443 â€¦ verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet â€¦ 200 OK\n",
      "LÃ¤nge: 434163450 (414M) [application/x-gzip]\n",
      "Wird in Â»polish_cyberbullying_bert_base.tgz.1Â« gespeichert.\n",
      "\n",
      "polish_cyberbullyin 100%[===================>] 414,05M  9,50MB/s    in 22s     \n",
      "\n",
      "2021-09-30 09:19:58 (18,8 MB/s) - Â»polish_cyberbullying_bert_base.tgz.1Â« gespeichert [434163450/434163450]\n",
      "\n",
      "x polish_cyberbullying_bert_base/\n",
      "x polish_cyberbullying_bert_base/added_tokens.json\n",
      "x polish_cyberbullying_bert_base/special_tokens_map.json\n",
      "x polish_cyberbullying_bert_base/config.json\n",
      "x polish_cyberbullying_bert_base/tokenizer.json\n",
      "x polish_cyberbullying_bert_base/merges.txt\n",
      "x polish_cyberbullying_bert_base/pytorch_model.bin\n",
      "a added_tokens.json\n",
      "a config.json\n",
      "a merges.txt\n",
      "a pytorch_model.bin\n",
      "a special_tokens_map.json\n",
      "a tokenizer.json\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**using the Hugging Face model**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Download Hugging Face Pretrained Model\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# MODEL = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "# model.save_pretrained('my_model')\n",
    "# tokenizer.save_pretrained('my_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# package the inference scrip and pre-trained model into .tar.gz format\n",
    "# !cd my_model && tar zcvf model.tar.gz * \n",
    "# !mv my_model/model.tar.gz ./model.tar.gz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**upload the mmodel to Amazon S3**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# upload pre-trained model to s3 bucket\n",
    "model_url = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/model\")\n",
    "print(f\"Uploading Model to {model_url}\")\n",
    "model_uri = S3Uploader.upload('model.tar.gz',model_url)\n",
    "print(f\"Uploaded model to {model_uri}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Batch Transform job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_uri, # configuration for loading model from Hub\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "    name=\"prosus-workshop-bert-batch\", # model and batch name\n",
    "   py_version='py36', # python version used\n",
    ")\n",
    "\n",
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=input_s3_path,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use batch transform when you:\n",
    "\n",
    "* Want to get inferences for an entire dataset and index them to serve inferences in real time\n",
    "* Don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "* Don't need the subsecond latency that SageMaker hosted endpoints provide\n",
    "\n",
    "You can also use batch transform to preprocess your data before using it to train a new model or generate inferences.\n",
    "The following diagram shows the workflow of a batch transform job:\n",
    "\n",
    "![batch-transform](../../imgs/batch-transform-v2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from ast import literal_eval\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"tweet_data.json.out\"\n",
    "output_path = s3_path_join(output_s3_path,output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')\n",
    "\n",
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "        \n",
    "# print results \n",
    "print(batch_transform_result[:20])"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}