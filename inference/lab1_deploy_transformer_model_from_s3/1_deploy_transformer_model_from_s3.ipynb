{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "known-leisure",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Deploy ðŸ¤— Transformers for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-hostel",
   "metadata": {},
   "source": [
    "Welcome to this getting started guide, we will use the new Hugging Face Inference DLCs and Amazon SageMaker Python SDK to deploy a transformer model for real-time inference. \n",
    "In this example we are going to deploy a trained Hugging Face Transformer model on to SageMaker for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-strength",
   "metadata": {},
   "source": [
    "## API - [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-meditation",
   "metadata": {},
   "source": [
    "Using the `transformers pipelines`, we designed an API, which makes it easy for you to benefit from all `pipelines` features. The API is oriented at the API of the [ðŸ¤—  Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html), meaning your inputs need to be defined in the `inputs` key and if you want additional supported `pipelines` parameters you can add them in the `parameters` key. Below you can find examples for requests. \n",
    "\n",
    "**text-classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "```\n",
    "**question-answering request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": {\n",
    "\t\t\"question\": \"What is used for inference?\",\n",
    "\t\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "**zero-shot classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
    "\t\"parameters\": {\n",
    "\t\t\"candidate_labels\": [\n",
    "\t\t\t\"refund\",\n",
    "\t\t\t\"legal\",\n",
    "\t\t\t\"faq\"\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-assets",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade\n",
    "# !pip install torch -q\n",
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-strengthening",
   "metadata": {},
   "source": [
    "#### **Note: Restart the notebook after installing the above packages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-semiconductor",
   "metadata": {},
   "source": [
    "## Deploy a Hugging Face Transformer model from S3 to SageMaker for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-beauty",
   "metadata": {},
   "source": [
    "There are two ways on how you can deploy you SageMaker trained Hugging Face model from S3. You can either deploy it after your training is finished or you can deploy it later using the `model_data` pointing to you saved model on s3.\n",
    "\n",
    "### Deploy the model directly after training\n",
    "\n",
    "If you deploy you model directly after training you need to make sure that all required files are saved in your training script, including the Tokenizer and the Model. \n",
    "```python\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "############ pseudo code start ############\n",
    "\n",
    "# create HuggingFace estimator for running training\n",
    "huggingface_estimator = HuggingFace(....)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(...)\n",
    "\n",
    "############ pseudo code end ############\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-round",
   "metadata": {},
   "source": [
    "### Download Hugging Face Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "MODEL = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model.save_pretrained('my_model')\n",
    "tokenizer.save_pretrained('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-intro",
   "metadata": {},
   "source": [
    "### Package the saved model to tar.gz formant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "veterinary-galaxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a config.json\n",
      "a pytorch_model.bin\n",
      "a special_tokens_map.json\n",
      "a tokenizer.json\n",
      "a tokenizer_config.json\n",
      "a vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!cd my_model && tar zcvf model.tar.gz * \n",
    "!mv my_model/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-complex",
   "metadata": {},
   "source": [
    "### Upload Pretrained Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "# get the s3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "# uploads a given file to S3.\n",
    "upload_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"lab1_model\")\n",
    "model_uri = S3Uploader.upload('model.tar.gz',upload_path)\n",
    "print(f\"Uploading Model to {upload_path}\")\n",
    "print(f\" model.tar.gz uploaded to {model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-consortium",
   "metadata": {},
   "source": [
    "### Deploy the model using `model_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_uri,  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.4xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-playing",
   "metadata": {},
   "source": [
    "**Architecture**\n",
    "\n",
    "The [Hugging Face Inference Toolkit for SageMaker](https://github.com/aws/sagemaker-huggingface-inference-toolkit) is an open-source library for serving Hugging Face transformer models on SageMaker. It utilizes the SageMaker Inference Toolkit for starting up the model server, which is responsible for handling inference requests. The SageMaker Inference Toolkit uses [Multi Model Server (MMS)](https://github.com/awslabs/multi-model-server) for serving ML models. It bootstraps MMS with a configuration and settings that make it compatible with SageMaker and allow you to adjust important performance parameters, such as the number of workers per model, depending on the needs of your scenario.\n",
    "\n",
    "![](../../imgs/hf-inference-toolkit.png)\n",
    "\n",
    "**Deploying a model using SageMaker hosting services is a three-step process:**\n",
    "\n",
    "1. **Create a model in SageMaker** â€”By creating a model, you tell SageMaker where it can find the model components. \n",
    "2. **Create an endpoint configuration for an HTTPS endpoint** â€”You specify the name of one or more models in production variants and the ML compute instances that you want SageMaker to launch to host each production variant.\n",
    "3. **Create an HTTPS endpoint** â€”Provide the endpoint configuration to SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration\n",
    "\n",
    "![](../../imgs/sm-endpoint.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. I love it!\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-asset",
   "metadata": {},
   "source": [
    "## Model Monitoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-genome",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
